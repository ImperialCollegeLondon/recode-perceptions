{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Places Categories and correlating with Object Detection Counts\n",
    "\n",
    "In this practical, we will make predictions of place categories using a pre-trained model and will then continue to extract object counts from the images.\n",
    "\n",
    "### Learning Objectives\n",
    "* Use pre-trained model for hold out test-set predictions\n",
    "* Load model for object detection and run inference on the image dataset\n",
    "* Examine the counts and correlations with the places scene category\n",
    "\n",
    "### Content\n",
    "1. Places Inference\n",
    "2. Tensorflow API\n",
    "3. Object Detection\n",
    "4. Analysis of Results\n",
    "\n",
    "The Tensorflow API and Object Detection section of this notebook were originally created for the Healthy Data Analytics Course at the School of Public Health, Imperial College London, 2021. The notebook was created by Barbara Metzler, Ricky Nathvani, Esra Suel and Emily Muller.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Places Inference\n",
    "In this tutorial we will be using a subset of the Places365 test dataset which we used to train our model. These images should already have been downloaded when running the datadownload.sh script. We will also use our pre-trained model from the previous section [2-cnn-training.md](2-cnn-training.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will allow us to import deep_cnn submodules\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are required to initialise the model as per the one which we wish to load. Make sure enter the same model base and number of classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_cnn.model_builder import MyCNN\n",
    "import torch\n",
    "\n",
    "# enter path to locally saved model\n",
    "model_path = '../outputs/models/resnet18_epochs50_lr1e-3_batch_156_test.pt'\n",
    "\n",
    "# Build model as per pre-trained model\n",
    "model = MyCNN(model_base='resnet101', n_classes=9)\n",
    "\n",
    "# load pre-trained model for fine-tuning\n",
    "checkpoint = torch.load(model_path)\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load the data into the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_cnn.dataset_generator import dataloader\n",
    "\n",
    "data_dir='input/places365standard_easyformat/places365_standard/'\n",
    "root_dir='/home/emily/phd/misc/recode'\n",
    "# create dataloader\n",
    "batch_size = 10\n",
    "params = {\n",
    "    \"batch_size\": batch_size,\n",
    "    \"shuffle\": False,\n",
    "    \"num_workers\": 4,\n",
    "    \"pin_memory\": False,\n",
    "    \"drop_last\": False,\n",
    "}\n",
    "test_dataloader, _, _ = dataloader(\n",
    "    data_dir, root_dir, \"resnet\", \"val\", params\n",
    ")\n",
    "\n",
    "print ('There are %s images in the test set' % str(test_dataloader.__len__()*batch_size) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run inference on the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "# INFERENCE\n",
    "model.train(False)\n",
    "y = np.zeros((int(len(test_dataloader)), batch_size))\n",
    "for i, tdata in enumerate(test_dataloader):\n",
    "    test_x = tdata[0]\n",
    "    tlabels = tdata[1].unsqueeze(dim=1)\n",
    "    toutputs = model.forward(test_x)\n",
    "\n",
    "    y[i] = toutputs.cpu().detach().numpy().argmax(axis=1)\n",
    "\n",
    "predictions = y.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the model predictions. Let's convert the numeric output values to category names. We will map them to the folder names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "img_folder='../input/places365standard_easyformat/places365_standard/val/'\n",
    "\n",
    "folders = Path(img_folder)\n",
    "dirs = sorted(\n",
    "    os.listdir((folders))\n",
    ")  # read all folder names in alphanumeric order\n",
    "\n",
    "categories = {}\n",
    "for i, x in enumerate(dirs):\n",
    "    categories[i] = dirs[i]  # map each item in list to name of directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting this altogether, we create a dataframe with image names and predictions. We will merge this with object detections later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make dataframe with text based predictions\n",
    "import pandas as pd\n",
    "images = Path(img_folder).rglob('*.jpg')\n",
    "\n",
    "prediction_df = pd.DataFrame([list(sorted(images)), predictions]).T\n",
    "prediction_df['category'] = prediction_df[1].apply(lambda x: categories[x])\n",
    "prediction_df['true'] = prediction_df[0].apply(lambda x: str(x).split('/')[-2])\n",
    "prediction_df['img'] = prediction_df[0].apply(lambda x: str(x).split('/')[-1])\n",
    "prediction_df['correct'] = prediction_df.apply(lambda x: 1 if x.true == x.category else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = prediction_df.sample(10)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise predictions\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.io import read_image\n",
    "import numpy as np \n",
    "\n",
    "for i, row in enumerate(sample.iterrows()):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    image = read_image(str(row[1][0])).numpy()\n",
    "    # axs[i].imshow(np.moveaxis(image, 0, -1))\n",
    "    # axs[i].set_title(row[1]['category'])\n",
    "    plt.imshow(np.moveaxis(image, 0, -1))\n",
    "    plt.title(row[1]['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did the model do? Let's take a look at where it is going wrong specifically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise bad predictions\n",
    "sample = prediction_df[prediction_df['correct'] == 0].sample(10)\n",
    "sample\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.io import read_image\n",
    "import numpy as np \n",
    "\n",
    "for i, row in enumerate(sample.iterrows()):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    image = read_image(str(row[1][0])).numpy()\n",
    "    # axs[i].imshow(np.moveaxis(image, 0, -1))\n",
    "    # axs[i].set_title(row[1]['category'])\n",
    "    plt.imshow(np.moveaxis(image, 0, -1))\n",
    "    plt.title('Pred: ' + row[1]['category'] + 'True: ' + row[1]['true'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where does the model go wrong? Does it seem that the model is wrong or the label might be incorrect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tensorflow API\n",
    "The TensorFlow Object Detection API is an open-source framework built on top of TensorFlow that makes it easy to construct, train and deploy object detection models. There are already pre-trained models in their framework which are referred to as Model Zoo. More info about the Tensorflow API (https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/install.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Installing the TensorFlow API\n",
    "\n",
    "This can be a bit tricky. We will begin by installing packages. Those which are not already present in the Jupyter environment using the !pip command. If you get an error for pip installation. Try adding *--user* to the end of each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U --pre tensorflow==\"2.*\"\n",
    "!pip install tf_slim\n",
    "!pip install pycocotools\n",
    "!pip install seaborn \n",
    "!pip install geopandas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get tensorflow/models by cloning the tensorflow/models github repository:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clone the tensorflow models github repository\n",
    "import pathlib\n",
    "import os\n",
    "if \"models\" in pathlib.Path.cwd().parts:\n",
    "  while \"models\" in pathlib.Path.cwd().parts:\n",
    "    os.chdir('..')\n",
    "elif not pathlib.Path('models').exists():\n",
    "  !git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to successfully install the object detection API, you will need to open a bash terminal. Make sure to run the ```%bash``` parts of the code in the terminal. Installation will take a few minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# cd models/research/\n",
    "# protoc object_detection/protos/*.proto --python_out=.\n",
    "# Install TensorFlow Object Detection API.\n",
    "# cp object_detection/packages/tf2/setup.py .\n",
    "# python -m pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the object detection utils, change into the models/research directory which we cloned from github above (your path might be different to adjust as necessary):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (os.getcwd())\n",
    "os.chdir('models/research')\n",
    "print (os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change back to the root directory afterwards\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "os.chdir('../..')\n",
    "print (os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great we now have access to TensorFlow Object Detection API where we will load the Object Detection model from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 3. Object detection\n",
    "\n",
    "In this section, we will introduce the object detection architecture, install the Tensorflow Object Detection API, load the model and run inference on the dataset.\n",
    "\n",
    "#### 3.1 Choosing a model architecture: MobileNet-SSD\n",
    "The two stage Object Detection algorithm named Faster R-CNN is made up of the region proposal network (RPN) and the detection network. This architecture continues to feature in some of the top performing networks (see https://paperswithcode.com/sota/object-detection-on-coco). \n",
    "\n",
    "However, in the interest of processing time and memory, we chose the <b>MobileNet-SSD for the tutorial</b>. \n",
    "\n",
    "Depending on your time/accuracy trade-off, you can choose the appropriate model from the TensorFlow API. If we want a high-speed model the single-shot detection (SSD) network works best. As its name suggests, the SSD network determines all bounding box probabilities in one go; hence, it is much faster than the two stage R-CNN.\n",
    "The SSD architecture is a single convolution network that learns to predict bounding box locations and classify these locations in one pass. Hence, SSD can be trained end-to-end. The SSD network consists of a base architecture (MobileNet in this case) followed by several convolution layers:\n",
    "\n",
    "![alt text](images/mobileNet-SSD-network-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SSD's operates on feature maps to detect the location of bounding boxes. Recall – a feature map is of the size Df * Df * M. For each feature map location, k bounding boxes are predicted. Each bounding box carries with it the following information:\n",
    " - 4 corner bounding box offset locations (cx, cy, w, h)\n",
    " - C class probabilities (c1, c2, …cp)\n",
    " \n",
    "SSD does not predict the shape of the box, rather just where the box is. The k bounding boxes each have a predetermined shape. The shapes are set prior to actual training. For example, in the figure above, there are 5 boxes, meaning k=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')   # Suppress Matplotlib warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Load Model\n",
    "\n",
    "Since we have successfully downloaded the Object Detection API, we will load the SSD MobileNet model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet shown below is used to download the pre-trained object detection model we shall use to perform inference. The particular detection algorithm we will use is the SSD mobilenet 640x640. More models can be found in the TensorFlow Object Detection Model Zoo (https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and extract model\n",
    "def download_model(model_name, model_date):\n",
    "    base_url = 'http://download.tensorflow.org/models/object_detection/tf2/'\n",
    "    model_file = model_name + '.tar.gz'\n",
    "    model_dir = tf.keras.utils.get_file(fname=model_name,\n",
    "                                        origin=base_url + model_date + '/' + model_file,\n",
    "                                        untar=True)\n",
    "    return str(model_dir)\n",
    "\n",
    "MODEL_DATE = '20200711'\n",
    "MODEL_NAME = 'ssd_mobilenet_v2_fpnlite_640x640_coco17_tpu-8'\n",
    "PATH_TO_MODEL_DIR = download_model(MODEL_NAME, MODEL_DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we download the labels file (.pbtxt). This contains a list of strings used to add the correct label to each detection (e.g. person). Since the pre-trained model we will use has been trained on the COCO dataset, we will need to download the labels file corresponding to this dataset, named mscoco_label_map.pbtxt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download labels file\n",
    "def download_labels(filename):\n",
    "    base_url = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/data/'\n",
    "    label_dir = tf.keras.utils.get_file(fname=filename,\n",
    "                                        origin=base_url + filename,\n",
    "                                        untar=False)\n",
    "    label_dir = pathlib.Path(label_dir)\n",
    "    return str(label_dir)\n",
    "\n",
    "LABEL_FILENAME = 'mscoco_label_map.pbtxt'\n",
    "PATH_TO_LABELS = download_labels(LABEL_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_SAVED_MODEL = PATH_TO_MODEL_DIR + \"/saved_model\"\n",
    "\n",
    "print('Loading model...', end='')\n",
    "start_time = time.time()\n",
    "\n",
    "# Load saved model and build the detection function\n",
    "detect_fn = tf.saved_model.load(PATH_TO_SAVED_MODEL)\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print('Done! Took {} seconds'.format(elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Load label map data (for plotting)\n",
    "Label maps join numerical indices to category names, so that when the CNN predicts 5, we know that this corresponds to the object: airplane. Here we use internal utility functions, but anything that returns a dictionary mapping integers to appropriate string labels would be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,\n",
    "                                                                    use_display_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Running Inference on the Image Dataset\n",
    "The code shown below loads an image, runs it through the detection model and visualizes the detection results, including the keypoints.\n",
    "\n",
    "Note that this will take a long time (several minutes) the first time you run this code due to tf.function’s trace-compilation — on subsequent runs (e.g. on new images), things will be faster.\n",
    "\n",
    "Print out detections[‘detection_boxes’] and try to match the box locations to the boxes in the image. Notice that coordinates are given in normalized form (i.e., in the interval [0, 1]).\n",
    "\n",
    "Set min_score_thresh to other values (between 0 and 1) to allow more detections in or to filter out more detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_folder = '../input/places365standard_easyformat/places365_standard/val/'\n",
    "\n",
    "def load_image_into_numpy_array(path):\n",
    "    \"\"\"Load an image from file into a numpy array.\n",
    "\n",
    "    Puts image into numpy array to feed into tensorflow graph.\n",
    "    Note that by convention we put it into a numpy array with shape\n",
    "    (height, width, channels), where channels=3 for RGB.\n",
    "\n",
    "    Args:\n",
    "      path: the file path to the image\n",
    "\n",
    "    Returns:\n",
    "      uint8 numpy array with shape (img_height, img_width, 3)\n",
    "    \"\"\"\n",
    "    return np.array(Image.open(path))\n",
    "\n",
    "def run_inference_for_single_image(model, image):\n",
    "    \n",
    "    image = load_image_into_numpy_array(image)\n",
    "    # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
    "    input_tensor = tf.convert_to_tensor(image)\n",
    "    # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
    "    input_tensor = input_tensor[tf.newaxis,...]\n",
    "\n",
    "    # Run inference\n",
    "    #model_fn = model.signatures['serving_default']\n",
    "    output_dict = model(input_tensor)\n",
    "\n",
    "    # All outputs are batches tensors.\n",
    "    # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
    "    # We're only interested in the first num_detections.\n",
    "    num_detections = int(output_dict.pop('num_detections'))\n",
    "    output_dict = {key:value[0, :num_detections].numpy() \n",
    "                 for key,value in output_dict.items()}\n",
    "    output_dict['num_detections'] = num_detections\n",
    "\n",
    "    # detection_classes should be ints.\n",
    "    output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
    "\n",
    "    # Handle models with masks:\n",
    "    if 'detection_masks' in output_dict:\n",
    "        # Reframe the the bbox mask to the image size.\n",
    "        detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "                  output_dict['detection_masks'], output_dict['detection_boxes'],\n",
    "                   image.shape[0], image.shape[1])      \n",
    "        detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
    "                                           tf.uint8)\n",
    "        output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
    "\n",
    "    return image, output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise inference on just 5 of the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for image_path in Path(img_folder).rglob('*.jpg'):\n",
    "    print('Running inference for {}... '.format(image_path), end='')\n",
    "    \n",
    "    # RUN INFERENCE\n",
    "    image_np, detections = run_inference_for_single_image(detect_fn, image_path)\n",
    "    \n",
    "    # Visualise Detection\n",
    "    image_np_with_detections = image_np.copy()\n",
    "\n",
    "    viz_utils.visualize_boxes_and_labels_on_image_array(\n",
    "          image_np_with_detections,\n",
    "          detections['detection_boxes'],\n",
    "          detections['detection_classes'],\n",
    "          detections['detection_scores'],\n",
    "          category_index,\n",
    "          use_normalized_coordinates=True,\n",
    "          max_boxes_to_draw=200,\n",
    "          min_score_thresh=.50,\n",
    "          agnostic_mode=False)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(image_np_with_detections)\n",
    "    print('Done')\n",
    "    i += 1\n",
    "    if i == 5:\n",
    "        break\n",
    "plt.show()\n",
    "\n",
    "#sphinx_gallery_thumbnail_number = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tensorflow API saves the results the in the detection object that contains multiple dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in detections:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will run inference on the entire dataset, storing the detection results in a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_on_all_images(img_folder='../input/places365standard_easyformat/places365_standard/val/'):\n",
    "    output_dicts = []\n",
    "    images = Path(img_folder).rglob('*.jpg')\n",
    "    corrupts = []\n",
    "    for ix, image in enumerate(images):\n",
    "        try:\n",
    "            im, output_dict = run_inference_for_single_image(detect_fn, image)\n",
    "        #print (images[ix])\n",
    "            output_dicts.append(output_dict)\n",
    "        except:\n",
    "            corrupts.append(image)\n",
    "        print ('Run inference on image %s' % ix)\n",
    "    return output_dicts, corrupts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import time\n",
    "start_time = time.time()\n",
    "det, corrupts = run_on_all_images(img_folder)\n",
    "end_time = time.time()\n",
    "print ('Takes %s seconds to run' % str(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Converting detected objects into DataFrame\n",
    "We will make the data more manageable for subsequent analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_scores = []\n",
    "detect_classes = []\n",
    "detect_ymin = []\n",
    "detect_xmin = []\n",
    "detect_ymax = []\n",
    "detect_xmax = []\n",
    "Id_list = []\n",
    "\n",
    "images = Path(img_folder).rglob('*.jpg')\n",
    "\n",
    "for img, output_dict in zip(images, det):\n",
    "    cut_off_scores = len(list(filter(lambda x: x >= 0.5, output_dict['detection_scores'])))\n",
    "    detect_score = []\n",
    "    detect_class = []\n",
    "    for j in range(cut_off_scores):\n",
    "        detect_score.append(output_dict['detection_scores'][j])\n",
    "        detect_class.append(output_dict['detection_classes'][j])\n",
    "    detect_scores.append(detect_score)\n",
    "    detect_classes.append(detect_class)\n",
    "    fname = os.path.basename(img)\n",
    "    Id_list.append(fname)\n",
    "    #Id_list.append()\n",
    "Detected_objects = pd.DataFrame(\n",
    "        {'Image': Id_list,\n",
    "         'Score': detect_scores,\n",
    "         'Class': detect_classes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Detected_objects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Detected_objects.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a dictionary with the labels from the COCO dataset\n",
    "coco_dict = category_index.values()\n",
    "coco = pd.DataFrame(coco_dict)\n",
    "coco = pd.Series(coco.name.values,index=coco.id).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Analysis of Results\n",
    "Let's perform some analysis to find out more about the features extracted from the model.\n",
    "\n",
    "We will first create a function to plot total object counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a row for each Class element\n",
    "lst_col = 'Class'\n",
    "df = pd.DataFrame({\n",
    "      col:np.repeat(Detected_objects[col].values, Detected_objects[lst_col].str.len())\n",
    "      for col in Detected_objects.columns.drop(lst_col)}\n",
    "    ).assign(**{lst_col:np.concatenate(Detected_objects[lst_col].values)})[Detected_objects.columns]\n",
    "df = df.drop('Score', axis = 1)\n",
    "\n",
    "# Maps class integer to object name\n",
    "df['Name'] = df['Class'].astype(int)\n",
    "df['Name'] = df['Name'].map(coco)      \n",
    "\n",
    "# plot total counts across all images\n",
    "df['Name'].value_counts().plot.bar(figsize=(20,20))\n",
    "plt.title('Object counts across all images')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the correlations between objects counts in a single image, across all images. What objects tend to appear together in the imagery?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot correlations between counts in a single image, across all images\n",
    "### POINT BASED ANALYSIS\n",
    "obj_df_img = pd.crosstab(df.Image, df.Name)\n",
    "obj_df_img = obj_df_img.groupby(['Image']).sum()\n",
    "obj_df_img.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_df_img.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "corr = obj_df_img.corr()\n",
    "ax = sns.heatmap(corr, cmap=\"YlGnBu\").set_title(\"Correlation matrix: pairwise correlation of the dected object classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlations are not weighted by the number of occurences. Consider plotting the heatmap normalised by object counts across all images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Correlation between scene category predictions and object detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge predictions to object detections\n",
    "obj_df_img['img'] = obj_df_img.index\n",
    "merged = prediction_df.merge(obj_df_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.groupby('category').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals = merged.groupby('category').sum()\n",
    "totals = totals[totals.columns[totals.sum()>5]] # only consider objects which are counted more than 5 times across all images. Easier to visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totals.plot.bar(figsize=(20,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What patterns do you notice with each of the categories and object counts? Does anything strike you as unexpected? Or do these predictions fit with your own reasoning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Further Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object Detections add a layer of interpretability to our analysis because they are localized labelled objects which tell us what make up the image. The CNN is opaque, unless we explore model-based methods of [interpretability](https://christophm.github.io/interpretable-ml-book/), typically called pixel-attribution methods.\n",
    "\n",
    "Try using pythons [scikitlearn](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning) library to build a classifier to predict class outcome using the object detection counts.\n",
    "\n",
    "Do you think it would perform better than the CNN or worse? If there are 10 classes, then an accuracy of more than 0.1 is better than random. Consider what else the CNN might be able to detect beyond the object detections made by the object detection network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "563a1e5b9548b49c7c99deadfdd1abc5fe80b0f8d242ed29eebd3f97b28807f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
